"""
Model Manager ƒë·ªÉ cache v√† qu·∫£n l√Ω c√°c models tr√°nh load l·∫°i nhi·ªÅu l·∫ßn
"""
import os
import threading
from typing import Dict, Any, Optional
from tool.embeddings import SentenceTransformerEmbedding, EmbeddingConfig
from tool.semantic_router import SemanticRouter, Route
from tool.semantic_router.sample import Sample
from setting import Settings

class ModelManager:
    """Singleton class ƒë·ªÉ qu·∫£n l√Ω v√† cache c√°c models"""
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
            
        self.settings = Settings.load_settings()
        self.models_cache: Dict[str, Any] = {}
        self._initialized = True
        
        print("üîß ModelManager initialized")
    
    def get_embedding_model(self, model_name: str = None) -> SentenceTransformerEmbedding:
        """
        L·∫•y embedding model t·ª´ cache ho·∫∑c load m·ªõi n·∫øu ch∆∞a c√≥
        """
        if model_name is None:
            model_name = "dangvantuan/vietnamese-document-embedding"
            
        cache_key = f"embedding_{model_name}"
        
        if cache_key not in self.models_cache:
            print(f"üöÄ Loading embedding model: {model_name}")
            config = EmbeddingConfig(name=model_name)
            embedding_model = SentenceTransformerEmbedding(config)
            self.models_cache[cache_key] = embedding_model
            print(f"‚úÖ Embedding model cached: {model_name}")
        else:
            print(f"üì¶ Using cached embedding model: {model_name}")
            
        return self.models_cache[cache_key]
    
    def get_semantic_router(self) -> SemanticRouter:
        """
        L·∫•y semantic router t·ª´ cache ho·∫∑c t·∫°o m·ªõi n·∫øu ch∆∞a c√≥
        """
        cache_key = "semantic_router"
        
        if cache_key not in self.models_cache:
            print("üöÄ Creating semantic router...")
            
            # L·∫•y embedding model
            embedding_tool = self.get_embedding_model()
            
            # T·∫°o c√°c routes
            routes = [
                Route(name="recruitment_incomplete", samples=Sample.recruitment_incomplete),
                Route(name="recruitment_complete", samples=Sample.recruitment_complete),
                Route(name="chitchat", samples=Sample.chitchatSample)
            ]
            
            # T·∫°o semantic router
            semantic_router = SemanticRouter(embedding=embedding_tool, routes=routes)
            self.models_cache[cache_key] = semantic_router
            print("‚úÖ Semantic router cached")
        else:
            print("üì¶ Using cached semantic router")
            
        return self.models_cache[cache_key]
    
    def get_llm_model(self, model_name: str = None):
        """
        L·∫•y LLM model t·ª´ cache (c√≥ th·ªÉ extend cho Ollama, etc.)
        """
        if model_name is None:
            model_name = self.settings.RAG_MODEL_ID
            
        cache_key = f"llm_{model_name}"
        
        if cache_key not in self.models_cache:
            print(f"üöÄ Loading LLM model: {model_name}")
            # TODO: Implement LLM loading logic
            # V√≠ d·ª• cho Ollama client
            from llms.ollama_llms import OllamaLLMs
            llm_model = OllamaLLMs(model_name=model_name)
            self.models_cache[cache_key] = llm_model
            print(f"‚úÖ LLM model cached: {model_name}")
        else:
            print(f"üì¶ Using cached LLM model: {model_name}")
            
        return self.models_cache[cache_key]
    
    def preload_models(self):
        """
        Preload t·∫•t c·∫£ models khi kh·ªüi ƒë·ªông ·ª©ng d·ª•ng
        """
        print("üöÄ Preloading all models...")
        
        # Preload embedding model
        self.get_embedding_model()
        
        # Preload semantic router
        self.get_semantic_router()
        
        # Preload LLM model (optional)
        # self.get_llm_model()
        
        print("‚úÖ All models preloaded successfully!")
    
    def clear_cache(self):
        """
        X√≥a cache models (ƒë·ªÉ free memory n·∫øu c·∫ßn)
        """
        self.models_cache.clear()
        print("üóëÔ∏è Model cache cleared")
    
    def get_cache_info(self) -> Dict[str, Any]:
        """
        L·∫•y th√¥ng tin v·ªÅ c√°c models ƒë√£ cache
        """
        return {
            "cached_models": list(self.models_cache.keys()),
            "cache_size": len(self.models_cache)
        }

# Global instance
model_manager = ModelManager()
